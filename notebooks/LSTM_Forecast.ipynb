{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Description**\n",
    "\n",
    "This notebook provides the code for creating, training, validating, and testing a rainfall–runoff forecasting model based on an LSTM architecture. The main difference with the \"LSTM_RainfallRunoof.ipynb\" notebook is that the model is trained to forecast the next \"n\" time steps. During testing, the model outputs a forecast vector for each time step in the testing period, containing predictions for the subsequent \"n\" time steps.\n",
    "\n",
    "***Authors:***\n",
    "- Eduardo Acuña Espinoza (eduardo.espinoza@kit.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import classes and functions from other files\n",
    "from hy2dl.datasetzoo import get_dataset\n",
    "from hy2dl.evaluation.metrics import forecast_NSE, forecast_PNSE\n",
    "from hy2dl.modelzoo import get_model\n",
    "from hy2dl.training.loss import nse_basin_averaged\n",
    "from hy2dl.utils.config import Config\n",
    "from hy2dl.utils.optimizer import Optimizer\n",
    "from hy2dl.utils.utils import set_random_seed, upload_to_device\n",
    "\n",
    "# colorblind friendly palette\n",
    "color_palette = {\"observed\": \"#377eb8\", \"simulated\": \"#4daf4a\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. Initialize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where all the information will be stored\n",
    "experiment_settings = {}\n",
    "\n",
    "# Experiment name\n",
    "experiment_settings[\"experiment_name\"] = \"LSTM_Forecast\"\n",
    "\n",
    "# paths to access the information\n",
    "experiment_settings[\"path_data\"] = \"../data/CAMELS_US\"\n",
    "experiment_settings[\"path_entities\"] = \"../data/basin_id/basins_camels_us_hourly_516.txt\"\n",
    "\n",
    "# dynamic forcings and target\n",
    "experiment_settings[\"dynamic_input\"] = [\n",
    "    \"convective_fraction\",\n",
    "    \"longwave_radiation\",\n",
    "    \"potential_energy\",\n",
    "    \"potential_evaporation\",\n",
    "    \"pressure\",\n",
    "    \"shortwave_radiation\",\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    "    \"total_precipitation\",\n",
    "    \"wind_u\",\n",
    "    \"wind_v\",\n",
    "]\n",
    "\n",
    "experiment_settings[\"forecast_input\"] = [\n",
    "    \"convective_fraction\",\n",
    "    \"longwave_radiation\",\n",
    "    \"potential_energy\",\n",
    "    \"potential_evaporation\",\n",
    "    \"pressure\",\n",
    "    \"shortwave_radiation\",\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    "    \"total_precipitation\",\n",
    "    \"wind_u\",\n",
    "    \"wind_v\",\n",
    "]\n",
    "\n",
    "experiment_settings[\"target\"] = [\"QObs(mm/h)\"]\n",
    "experiment_settings[\"forcings\"] = [\"nldas_hourly\"]\n",
    "\n",
    "# static attributes that will be used. If one is not using static_inputs, initialize the variable as an empty list.\n",
    "experiment_settings[\"static_input\"] = [\n",
    "    \"elev_mean\",\n",
    "    \"slope_mean\",\n",
    "    \"area_gages2\",\n",
    "    \"frac_forest\",\n",
    "    \"lai_max\",\n",
    "    \"lai_diff\",\n",
    "    \"gvf_max\",\n",
    "    \"gvf_diff\",\n",
    "    \"soil_depth_pelletier\",\n",
    "    \"soil_depth_statsgo\",\n",
    "    \"soil_porosity\",\n",
    "    \"soil_conductivity\",\n",
    "    \"max_water_content\",\n",
    "    \"sand_frac\",\n",
    "    \"silt_frac\",\n",
    "    \"clay_frac\",\n",
    "    \"carbonate_rocks_frac\",\n",
    "    \"geol_permeability\",\n",
    "    \"p_mean\",\n",
    "    \"pet_mean\",\n",
    "    \"aridity\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"high_prec_dur\",\n",
    "    \"low_prec_freq\",\n",
    "    \"low_prec_dur\",\n",
    "]\n",
    "\n",
    "# time periods\n",
    "experiment_settings[\"training_period\"] = [\"1990-10-01 00:00:00\", \"2003-09-30 23:00:00\"]\n",
    "experiment_settings[\"validation_period\"] = [\"2003-10-01 00:00:00\", \"2008-09-30 23:00:00\"]\n",
    "experiment_settings[\"testing_period\"] = [\"2008-10-01 00:00:00\", \"2018-09-30 23:00:00\"]\n",
    "\n",
    "# model configuration\n",
    "experiment_settings[\"hidden_size\"] = 128\n",
    "experiment_settings[\"batch_size_training\"] = 256\n",
    "experiment_settings[\"batch_size_evaluation\"] = 1024\n",
    "experiment_settings[\"epochs\"] = 12\n",
    "experiment_settings[\"dropout_rate\"] = 0.4\n",
    "experiment_settings[\"learning_rate\"] = {1: 1e-3, 5: 5e-4, 7: 1e-4}\n",
    "experiment_settings[\"validate_every\"] = 3\n",
    "experiment_settings[\"validate_n_random_basins\"] = -1\n",
    "\n",
    "experiment_settings[\"seq_length_hindcast\"] = 364 * 24\n",
    "experiment_settings[\"custom_seq_processing\"] = {\n",
    "    \"1D\": {\n",
    "        \"n_steps\": 360,\n",
    "        \"freq_factor\": 24,\n",
    "    },\n",
    "    \"1h\": {\"n_steps\": 4 * 24, \"freq_factor\": 1},\n",
    "}\n",
    "experiment_settings[\"seq_length_forecast\"] = 24\n",
    "experiment_settings[\"predict_last_n\"] = 24\n",
    "\n",
    "experiment_settings[\"unique_prediction_blocks\"] = False\n",
    "experiment_settings[\"dynamic_embedding\"] = {\"hiddens\": [10, 10, 10]}\n",
    "experiment_settings[\"custom_seq_processing_flag\"] = True\n",
    "\n",
    "# device to train the model\n",
    "experiment_settings[\"device\"] = \"cuda:0\"\n",
    "experiment_settings[\"num_workers\"] = 4\n",
    "\n",
    "# define random seed\n",
    "experiment_settings[\"random_seed\"] = 110\n",
    "\n",
    "# dataset\n",
    "experiment_settings[\"dataset\"] = \"hourly_camels_us\"\n",
    "# model\n",
    "experiment_settings[\"model\"] = \"forecast_LSTM\"\n",
    "experiment_settings[\"initial_forget_bias\"] = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read experiment settings\n",
    "config = Config(experiment_settings)\n",
    "config.init_experiment()\n",
    "config.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2. Create datasets and dataloaders used to train/validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset class\n",
    "Dataset = get_dataset(config)\n",
    "\n",
    "# Dataset training\n",
    "config.logger.info(f\"Loading training data from {config.dataset} dataset\")\n",
    "total_time = time.time()\n",
    "\n",
    "training_dataset = Dataset(cfg=config, time_period=\"training\")\n",
    "\n",
    "training_dataset.calculate_basin_std()\n",
    "training_dataset.calculate_global_statistics(save_scaler=True)\n",
    "training_dataset.standardize_data()\n",
    "\n",
    "config.logger.info(f\"Number of entities with valid samples: {len(training_dataset.df_ts)}\")\n",
    "config.logger.info(\n",
    "    \"Time required to process {} entities: {}\".format(\n",
    "        len(training_dataset.df_ts),\n",
    "        datetime.timedelta(seconds=int(time.time() - total_time))\n",
    "    )\n",
    ")\n",
    "config.logger.info(f\"Number of valid training samples: {len(training_dataset)}\\n\")\n",
    "\n",
    "# Dataloader training\n",
    "train_loader = DataLoader(\n",
    "    dataset=training_dataset,\n",
    "    batch_size=config.batch_size_training,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=training_dataset.collate_fn,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "# Print details of a loader´s sample to check that the format is correct\n",
    "config.logger.info(\"Details training dataloader\".center(60, \"-\"))\n",
    "config.logger.info(f\"Batch structure (number of batches: {len(train_loader)})\")\n",
    "config.logger.info(f\"{'Key':^30}|{'Shape':^30}\")\n",
    "# config.logger.info(\"-\" * 60)\n",
    "# Loop through the sample dictionary and print the shape of each element\n",
    "for key, value in next(iter(train_loader)).items():\n",
    "    if key.startswith((\"x_d\", \"x_conceptual\")):\n",
    "        config.logger.info(f\"{key}\")\n",
    "        for i, v in value.items():\n",
    "            config.logger.info(f\"{i:^30}|{str(v.shape):^30}\")\n",
    "    else:\n",
    "        config.logger.info(f\"{key:<30}|{str(value.shape):^30}\")\n",
    "\n",
    "config.logger.info(\"\")  # prints a blank line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In evaluation (validation and testing) we will create an individual dataset per basin\n",
    "config.logger.info(f\"Loading validation data from {config.dataset} dataset\")\n",
    "entities_ids = np.loadtxt(config.path_entities_validation, dtype=\"str\").tolist()\n",
    "iterator = tqdm(\n",
    "    [entities_ids] if isinstance(entities_ids, str) else entities_ids,\n",
    "    desc=\"Processing entities\",\n",
    "    unit=\"entity\",\n",
    "    ascii=True,\n",
    ")\n",
    "\n",
    "total_time = time.time()\n",
    "validation_dataset = {}\n",
    "for entity in iterator:\n",
    "    dataset = Dataset(cfg=config, time_period=\"validation\", check_NaN=False, entities_ids=entity)\n",
    "\n",
    "    dataset.scaler = training_dataset.scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    validation_dataset[entity] = dataset\n",
    "\n",
    "config.logger.info(\n",
    "    f\"Time required to process {len(iterator)} entities: {datetime.timedelta(seconds=int(time.time() - total_time))}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "set_random_seed(cfg=config)\n",
    "model = get_model(config).to(config.device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer(cfg=config, model=model)\n",
    "\n",
    "# Training report structure\n",
    "config.logger.info(\"Training model\".center(60, \"-\"))\n",
    "config.logger.info(f\"{'':^16}|{'Trainining':^21}|{'Validation':^21}|\")\n",
    "config.logger.info(f\"{'Epoch':^5}|{'LR':^10}|{'Loss':^10}|{'Time':^10}|{'Metric':^10}|{'Time':^10}|\")\n",
    "\n",
    "total_time = time.time()\n",
    "# Loop through epochs\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    train_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    # Training -------------------------------------------------------------------------------------------------------\n",
    "    model.train()\n",
    "    # Loop through the different batches in the training dataset\n",
    "    iterator = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch}/{config.epochs}. Training\",\n",
    "        unit=\"batches\",\n",
    "        ascii=True,\n",
    "        leave=False,\n",
    "    )\n",
    "\n",
    "    for idx, sample in enumerate(iterator):\n",
    "        # reach maximum iterations per epoch\n",
    "        if config.max_updates_per_epoch is not None and idx >= config.max_updates_per_epoch:\n",
    "            break\n",
    "\n",
    "        sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "        optimizer.optimizer.zero_grad()  # sets gradients to zero\n",
    "\n",
    "        # Forward pass of the model\n",
    "        pred = model(sample)\n",
    "        # Calcuate loss\n",
    "        loss = nse_basin_averaged(y_sim=pred[\"y_hat\"], y_obs=sample[\"y_obs\"], per_basin_target_std=sample[\"std_basin\"])\n",
    "\n",
    "        # Backpropagation (calculate gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters (e.g, weights and biases)\n",
    "        optimizer.clip_grad_and_step(epoch, idx)\n",
    "\n",
    "        # Keep track of the loss evolution\n",
    "        running_loss += (loss.detach().item() - running_loss) / (idx + 1)\n",
    "        iterator.set_postfix({\"average loss\": f\"{running_loss:.3f}\"})\n",
    "\n",
    "        # remove elements from cuda to free memory\n",
    "        del sample, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # training report\n",
    "    lr = optimizer.optimizer.param_groups[0][\"lr\"]\n",
    "    train_duration = str(datetime.timedelta(seconds=int(time.time() - train_time)))\n",
    "    report = f\"{epoch:^5}|{lr:^10.5f}|{running_loss:^10.3f}|{train_duration:^10}|\"\n",
    "\n",
    "    # Validation -----------------------------------------------------------------------------------------------------\n",
    "    if epoch % config.validate_every == 0:\n",
    "        val_time = time.time()\n",
    "        model.eval()\n",
    "        validation_results = {}\n",
    "        with torch.no_grad():\n",
    "            # If we define validate_n_random_basins as 0 or negative, we take all the basins. Otherwise, we randomly \n",
    "            # select the number of basins defined in validate_n_random_basins\n",
    "            if config.validate_n_random_basins <= 0:\n",
    "                validation_basin_ids = validation_dataset.keys()\n",
    "            else:\n",
    "                validation_basin_ids = random.sample(list(validation_dataset.keys()), config.validate_n_random_basins)\n",
    "\n",
    "            # Go through each basin\n",
    "            iterator = tqdm(\n",
    "                validation_basin_ids,\n",
    "                desc=f\"Epoch {epoch}/{config.epochs}. Validation\",\n",
    "                unit=\"basins\",\n",
    "                ascii=True,\n",
    "                leave=False,\n",
    "            )\n",
    "\n",
    "            for basin in iterator:\n",
    "                loader = DataLoader(\n",
    "                    dataset=validation_dataset[basin],\n",
    "                    batch_size=config.batch_size_evaluation,\n",
    "                    shuffle=False,\n",
    "                    drop_last=False,\n",
    "                    collate_fn=validation_dataset[basin].collate_fn,\n",
    "                    num_workers=config.num_workers,\n",
    "                )\n",
    "\n",
    "                dates, observed_values, simulated_values = [], [], []\n",
    "                for i, sample in enumerate(loader):\n",
    "                    sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "                    pred = model(sample)\n",
    "                    # backtransformed information\n",
    "                    y_sim = (\n",
    "                        pred[\"y_hat\"] * validation_dataset[basin].scaler[\"y_std\"].to(config.device)\n",
    "                    )\n",
    "                    y_mean = validation_dataset[basin].scaler[\"y_mean\"].to(config.device)\n",
    "                    y_sim = y_sim + y_mean\n",
    "\n",
    "                    # Join the results from the different batches\n",
    "                    dates.extend(sample[\"date_issue_fc\"])\n",
    "                    observed_values.extend(sample[\"persistent_q\"].cpu().detach().numpy())\n",
    "                    simulated_values.append(y_sim.cpu().detach().numpy())\n",
    "                    if i == len(loader) - 1:\n",
    "                        dates.extend(sample[\"date\"][-1, :])\n",
    "                        observed_values.extend(sample[\"y_obs\"][-1, :].cpu().detach().numpy())\n",
    "\n",
    "                    # remove from cuda\n",
    "                    del sample, pred, y_sim\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # Construct dataframe with observed and simulated values\n",
    "                df = pd.DataFrame(index=dates)\n",
    "                df[\"Observed\"] = np.concatenate(observed_values, axis=0)\n",
    "                y_sim = np.squeeze(np.concatenate(simulated_values, axis=0), -1)\n",
    "                y_sim = np.concatenate((y_sim, np.full([y_sim.shape[1], y_sim.shape[1]], np.nan)), axis=0)\n",
    "                df[[f\"lead_time_{i + 1}\" for i in range(y_sim.shape[1])]] = y_sim\n",
    "\n",
    "                validation_results[basin] = df\n",
    "\n",
    "            # average loss validation\n",
    "            loss_validation = forecast_NSE(results=validation_results).median().mean()\n",
    "            report += f\"{loss_validation:^10.3f}|{str(datetime.timedelta(seconds=int(time.time() - val_time))):^10}|\"\n",
    "\n",
    "    # No validation\n",
    "    else:\n",
    "        report += f\"{'':^10}|{'':^10}|\"\n",
    "\n",
    "    # Print report and save model\n",
    "    config.logger.info(report)\n",
    "    torch.save(model.state_dict(), config.path_save_folder / \"model\" / f\"model_epoch_{epoch}\")\n",
    "    # modify learning rate\n",
    "    optimizer.update_optimizer_lr(epoch=epoch)\n",
    "\n",
    "# print total training time\n",
    "config.logger.info(f\"Total training time: {datetime.timedelta(seconds=int(time.time() - total_time))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I already trained an LSTM I can re-construct the model. I just need to define the epoch for which I want to\n",
    "# re-construct the model\n",
    "# model = get_model(config).to(config.device)\n",
    "# model.load_state_dict(torch.load(config.path_save_folder / \"model\" / \"model_epoch_20\", map_location=config.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previously generated scaler\n",
    "with open(config.path_save_folder / \"scaler.pickle\", \"rb\") as file:\n",
    "    scaler = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In evaluation (validation and testing) we will create an individual dataset per basin\n",
    "config.logger.info(f\"Loading testing data from {config.dataset} dataset\")\n",
    "\n",
    "entities_ids = np.loadtxt(config.path_entities_testing, dtype=\"str\").tolist()\n",
    "iterator = tqdm(\n",
    "    [entities_ids] if isinstance(entities_ids, str) else entities_ids,\n",
    "    desc=\"Processing entities\",\n",
    "    unit=\"entity\",\n",
    "    ascii=True,\n",
    ")\n",
    "\n",
    "total_time = time.time()\n",
    "testing_dataset = {}\n",
    "for entity in iterator:\n",
    "    dataset = Dataset(cfg=config, time_period=\"testing\", check_NaN=False, entities_ids=entity)\n",
    "\n",
    "    dataset.scaler = scaler\n",
    "    dataset.standardize_data(standardize_output=False)\n",
    "    testing_dataset[entity] = dataset\n",
    "\n",
    "config.logger.info(\n",
    "    f\"Time required to process {len(iterator)} entities: {datetime.timedelta(seconds=int(time.time() - total_time))}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.logger.info(\"Testing model\".center(60, \"-\"))\n",
    "total_time = time.time()\n",
    "\n",
    "model.eval()\n",
    "test_results = {}\n",
    "with torch.no_grad():\n",
    "    # Go through each basin\n",
    "    iterator = tqdm(testing_dataset, desc=\"Testing\", unit=\"basins\", ascii=True)\n",
    "    for basin in iterator:\n",
    "        loader = DataLoader(\n",
    "            dataset=testing_dataset[basin],\n",
    "            batch_size=config.batch_size_evaluation,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            collate_fn=testing_dataset[basin].collate_fn,\n",
    "            num_workers=config.num_workers,\n",
    "        )\n",
    "\n",
    "        dates, simulated_values, observed_values = [], [], []\n",
    "        for i, sample in enumerate(loader):\n",
    "            sample = upload_to_device(sample, config.device)  # upload tensors to device\n",
    "            pred = model(sample)\n",
    "            # backtransformed information\n",
    "            y_sim = pred[\"y_hat\"] * testing_dataset[basin].scaler[\"y_std\"].to(config.device) + testing_dataset[\n",
    "                basin\n",
    "            ].scaler[\"y_mean\"].to(config.device)\n",
    "\n",
    "            # Join the results from the different batches\n",
    "            dates.extend(sample[\"date_issue_fc\"])\n",
    "            observed_values.extend(sample[\"persistent_q\"].cpu().detach().numpy())\n",
    "            simulated_values.append(y_sim.cpu().detach().numpy())\n",
    "            if i == len(loader) - 1:\n",
    "                dates.extend(sample[\"date\"][-1, :])\n",
    "                observed_values.extend(sample[\"y_obs\"][-1, :].cpu().detach().numpy())\n",
    "\n",
    "            # remove from cuda\n",
    "            del sample, pred, y_sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Construct dataframe with observed and simulated values\n",
    "        df = pd.DataFrame(index=dates)\n",
    "        df[\"Observed\"] = np.concatenate(observed_values, axis=0)\n",
    "        y_sim = np.squeeze(np.concatenate(simulated_values, axis=0), -1)\n",
    "        y_sim = np.concatenate((y_sim, np.full([y_sim.shape[1], y_sim.shape[1]], np.nan)), axis=0)\n",
    "        df[[f\"lead_time_{i + 1}\" for i in range(y_sim.shape[1])]] = y_sim\n",
    "\n",
    "        # Save the dataframe in a basin-indexed dictionary\n",
    "        test_results[basin] = df\n",
    "\n",
    "# Save results as a pickle file\n",
    "with open(config.path_save_folder / \"test_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(test_results, f)\n",
    "\n",
    "config.logger.info(f\"Total testing time: {datetime.timedelta(seconds=int(time.time() - total_time))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5. Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NSE = forecast_NSE(results=test_results)\n",
    "df_PNSE = forecast_PNSE(results=test_results)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 12), sharex=True)  # Share x-axis\n",
    "\n",
    "# First subplot: NSE\n",
    "axes[0].boxplot(\n",
    "    df_NSE.dropna().values,\n",
    "    widths=0.8,\n",
    "    positions=np.arange(len(df_NSE.columns)) + 1,\n",
    "    showfliers=False,\n",
    ")\n",
    "\n",
    "medians = df_NSE.median(axis=0).values\n",
    "for i, median in enumerate(medians):\n",
    "    axes[0].text(\n",
    "        i + 1,\n",
    "        median,\n",
    "        f\"{median:.2f}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "axes[0].set_ylabel(\"NSE\", fontsize=16, fontweight=\"bold\")\n",
    "axes[0].yaxis.set_major_formatter(ticker.FormatStrFormatter(\"%.1f\"))\n",
    "axes[0].tick_params(axis=\"both\", labelsize=14)\n",
    "\n",
    "# Second subplot: Boxplot\n",
    "axes[1].boxplot(\n",
    "    df_PNSE.dropna().values,\n",
    "    widths=0.8,\n",
    "    positions=np.arange(len(df_PNSE.columns)) + 1,\n",
    "    showfliers=False,\n",
    ")\n",
    "\n",
    "# Add median values as text\n",
    "medians = df_PNSE.median(axis=0).values\n",
    "for i, median in enumerate(medians):\n",
    "    axes[1].text(\n",
    "        i + 1,\n",
    "        median,\n",
    "        f\"{median:.2f}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "axes[1].set_xlabel(\"Lead time [h]\", fontsize=16, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"PNSE\", fontsize=16, fontweight=\"bold\")\n",
    "axes[1].tick_params(axis=\"both\", labelsize=16)\n",
    "axes[1].yaxis.set_major_formatter(ticker.FormatStrFormatter(\"%.1f\"))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random basin and date\n",
    "basin_to_analyze = random.sample(list(test_results.keys()), 1)[0]\n",
    "\n",
    "# Establish period of interest as 48 hours before and after a random peak\n",
    "date = random.sample(list(test_results[basin_to_analyze].Observed.nlargest(200).index), 1)[0]\n",
    "window_size = 48\n",
    "start_date = date - pd.Timedelta(hours=window_size)\n",
    "end_date = date + pd.Timedelta(hours=window_size)\n",
    "period_of_interest = [start_date, end_date]\n",
    "\n",
    "# Filter the results\n",
    "df_period_of_interest = test_results[basin_to_analyze].loc[period_of_interest[0] : period_of_interest[1], :]\n",
    "\n",
    "# Precipitation\n",
    "df_prec = (\n",
    "    testing_dataset[basin_to_analyze]\n",
    "    .df_ts[basin_to_analyze]\n",
    "    .loc[period_of_interest[0] : period_of_interest[1], [\"total_precipitation\"]]\n",
    ")\n",
    "\n",
    "# Create figure\n",
    "fig, ax1 = plt.subplots(figsize=(15, 7.5))\n",
    "\n",
    "# Observe series\n",
    "ax1.plot(\n",
    "    df_period_of_interest[\"Observed\"],\n",
    "    label=\"Observed discharge\",\n",
    "    color=color_palette[\"observed\"],\n",
    "    linewidth=3,\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "# Simulated forecasted series\n",
    "for i in range(df_period_of_interest.shape[0] - 1):\n",
    "    time_slide = pd.date_range(\n",
    "        start=df_period_of_interest.index[i + 1], periods=df_period_of_interest.shape[1] - 1, freq=\"h\"\n",
    "    )\n",
    "\n",
    "    forecast = df_period_of_interest.iloc[i, 1:].values\n",
    "    ax1.plot(time_slide, forecast, alpha=0.5, linestyle=\"--\")\n",
    "\n",
    "# Precipitation\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(df_prec.index, df_prec.squeeze().values, color=\"skyblue\", width=0.02, label=\"Precipitation\", alpha=0.8)\n",
    "\n",
    "# Format plot\n",
    "ax1.set_xlabel(\"Date\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.tick_params(axis=\"x\", labelsize=14)\n",
    "ax1.set_ylabel(\"Discharge [mm/h]\", fontsize=16, fontweight=\"bold\")\n",
    "ax1.tick_params(axis=\"y\", labelsize=14)\n",
    "ax1.set_title(\"Forecasted discharge\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "ax2.set_ylabel(\"Precipitation (mm)\", fontsize=16, fontweight=\"bold\")\n",
    "ax2.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# Create legend\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"upper center\", bbox_to_anchor=(0.2, -0.1), ncol=5, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "hy2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
